{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KBS Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od_tRfdCiY5b"
      },
      "source": [
        "**R181603K  NGONDZASHE L CHIKUVADZE**\n",
        "\n",
        "**R182562C  ONWELL J SHOKO**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tL1NL38rXNP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "277e0cd1-4d47-4b20-f30d-aa98141bf34e"
      },
      "source": [
        "!pip install pyngrok"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.7/dist-packages (5.0.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (3.13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKFQX60AZpUP"
      },
      "source": [
        "!pip install -q streamlit"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVBr4ERZrfzI"
      },
      "source": [
        "from pyngrok import ngrok"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTwHVaAmrjVD",
        "outputId": "0b02815d-343b-4741-b531-e557d97338d9"
      },
      "source": [
        "!ngrok authtoken 1npJviy2rm2H1RzUbevBUv2GLGw_ep9y73PeZMfA5zeSBzzs"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CI_thqItr2mt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d21f63c-3145-490e-d5fd-eebeef7f8e9e"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr3UrbVs0bYE"
      },
      "source": [
        "!pip install -q python-crfsuite"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Iu-TTbR1OFs"
      },
      "source": [
        "!pip install -q sklearn-pycrfsuite"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlD_6wAqZg-K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c8b8403-b1b8-422a-af0f-f4dbd75eed1a"
      },
      "source": [
        "%%writefile qg.py\n",
        "import streamlit as st\n",
        "import string\n",
        "import pycrfsuite\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tag import CRFTagger\n",
        "from nltk import word_tokenize,sent_tokenize\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import time\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "ct = CRFTagger()\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "PAGE_CONFIG = {\"page_title\":\"StColab.io\", \"page_icon\":\":smiley:\",\"layout\":\"centered\"}\n",
        "st.set_page_config(\"PAGE_CONFIG\")\n",
        "\n",
        "st.title(\"Deep Learning - Question Generation\")\n",
        "st.header(\"R181603K\")\n",
        "st.header(\"R182562C\")\n",
        "\n",
        "def main():\n",
        "  train_set  = pd.read_csv('/content/drive/My Drive/Colab Notebooks/kbs_project/Train_and_test_data/train.txt',sep=' ',names=['word','Brill','tag']).drop('Brill',1)\n",
        "  train_set = [[tuple(x) for x in train_set.values]]\n",
        "  sentence = ' '.join(open('/content/drive/My Drive/Colab Notebooks/kbs_project/Text_Passages2.txt','r').readlines()).rstrip(\"\\n\")\n",
        "\n",
        "  def process_text(passage):\n",
        "    exclude_set = set(['“','”',':'])\n",
        "    no_punctuation = [char for char in passage if char not in string.punctuation + \"“”.\"]\n",
        "    no_punctuation = ''.join(no_punctuation)\n",
        "    no_punctuation = [word for word in no_punctuation.split() if word.lower() not in stopwords.words('english')]\n",
        "    return no_punctuation\n",
        "\n",
        "  nltk.download('stopwords')\n",
        "  sentence = process_text(sentence)\n",
        "  sentence = ' '.join(sentence)  \n",
        "  nltk.download('punkt')\n",
        "\n",
        "  sentences = sent_tokenize(sentence)\n",
        "  word_list = [[]]\n",
        "  word_list.clear()\n",
        "  wlist = []\n",
        "  i = 0\n",
        "  j = 0\n",
        "\n",
        "  for sent in sentences:\n",
        "      for word in sent.split():\n",
        "          wlist.append(word)\n",
        "          j += 1\n",
        "      wlist.copy\n",
        "      word_list.append(wlist.copy())\n",
        "      wlist.clear()\n",
        "      i += 1\n",
        "\n",
        "  ct.set_model_file(model_file='/content/drive/My Drive/Colab Notebooks/kbs_project/model.crf.tagger')\n",
        "  word_list = ct.tag_sents(word_list)\n",
        "\n",
        "  test_set = pd.read_csv('/content/drive/My Drive/Colab Notebooks/kbs_project/Train_and_test_data/test.txt', sep=' ', names=['words','Brill','tag'], index_col=False).drop(['Brill','tag'],1)\n",
        "  test_set = test_set.values.tolist()\n",
        "  ct.tag_sents(test_set)\n",
        "\n",
        "  test_setEval = pd.read_csv('/content/drive/My Drive/Colab Notebooks/kbs_project/Train_and_test_data/test.txt', sep=' ', names=['words','Brill','tag']).drop('Brill',1)\n",
        "  test_setEval = [[tuple(x) for x in test_setEval.values]]\n",
        "  ct.evaluate(test_setEval)\n",
        "  word_list = np.array(word_list)\n",
        "  word_list = np.reshape(word_list, (-1,2))\n",
        "\n",
        "  def findSubject(pattern):\n",
        "      if pattern[0] == 'NP' and pattern[1] == 'VP' and pattern[2] == 'NP':\n",
        "          return True\n",
        "      else:\n",
        "          return False\n",
        "  pattern = []\n",
        "  subjects = []\n",
        "  word_list_length = len(word_list)\n",
        "\n",
        "  for i in range(0, len(word_list)):\n",
        "      try:\n",
        "          if((word_list_length - i == 2)):\n",
        "              break\n",
        "          else:\n",
        "              pattern = [word_list[i][1].split('-')[1], word_list[i+1][1].split('-')[1], word_list[i+2][1].split('-')[1]]\n",
        "              if findSubject(pattern):\n",
        "                  #print('Potential Subject Found at index {}'.format(i))\n",
        "                  #print('Subject -> {} => {} {}'.format(word_list[i][0], word_list[i+1][0], word_list[i+2][0]))\n",
        "                  sub = [word_list[i][0],word_list[i+1][0],word_list[i+2][0]]\n",
        "                  subjects.append(' '.join(sub))\n",
        "                  print('--------------------')\n",
        "      except IndexError:\n",
        "          #print(\"Out of Index\")\n",
        "          break;\n",
        "\n",
        "  subjects = np.array(subjects)\n",
        "  subjects = np.reshape(subjects, (-1,1))\n",
        "  refferingDataFrame = pd.DataFrame(columns=('Word_POS-TAG_Person','nullColumn'))\n",
        "\n",
        "  for i in range(0,len(subjects)):\n",
        "      for j in range(0,len(subjects[i][0].split(\" \"))):\n",
        "          wordBag = subjects[i][0].split(\" \")\n",
        "          pos = ct.tag([wordBag[j]])\n",
        "          person = [ent.label_ for ent in nlp(pos[0][0]).ents]\n",
        "          person = ' '.join(person)\n",
        "          if not person:\n",
        "              person = \"\"\n",
        "          refferingDataFrame.loc[len(refferingDataFrame)] = [(pos[0][0] +\"_\"+ pos[0][1].split('-')[1] +\"_\"+ person).rstrip('_'), \"\"]\n",
        "\n",
        "  lstm = nn.LSTM(3, 3)\n",
        "  inputs = [autograd.Variable(torch.randn(1, 3)) for _ in range(5)]\n",
        "  hidden = (autograd.Variable(torch.randn(1, 1, 3)), autograd.Variable(torch.randn(1, 1, 3)))\n",
        "\n",
        "  for i in inputs:\n",
        "      out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
        "      \n",
        "  inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
        "  hidden = (autograd.Variable(torch.randn(1, 1, 3)), autograd.Variable(torch.randn(1, 1, 3)))\n",
        "  out, hidden = lstm(inputs, hidden)\n",
        "  #print(out)\n",
        "  #print(hidden)\n",
        "\n",
        "  def prepare_sequence(seq, to_ix):\n",
        "      idxs = [to_ix[w] for w in seq]\n",
        "      tensor = torch.LongTensor(idxs)\n",
        "      return autograd.Variable(tensor)\n",
        "  unprocessed_data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/kbs_project/Train_and_test_data/LSTM_train_set.csv',header=None)\n",
        "\n",
        "  t_data_for_lstm = []\n",
        "  training_data = []\n",
        "  for phrase in unprocessed_data.itertuples():\n",
        "      t_data_for_lstm.append(list(zip([[phrase[1]]], [[phrase[2]]])))\n",
        "\n",
        "  for i in range(0, len(t_data_for_lstm)):\n",
        "      training_data.append(t_data_for_lstm[i][0])\n",
        "      \n",
        "  word_to_ix = {}\n",
        "  tag_to_ix = {}\n",
        "\n",
        "  for sent, tags in training_data:\n",
        "      for word in sent:\n",
        "          if word not in word_to_ix:\n",
        "              word_to_ix[word] = len(word_to_ix)\n",
        "      for word1 in tags:\n",
        "          if word1 not in tag_to_ix:\n",
        "              tag_to_ix[word1] = len(tag_to_ix)\n",
        "                              \n",
        "  EMBEDDING_DIM = 6\n",
        "  HIDDEN_DIM = 6\n",
        "\n",
        "  # The LSTM model\n",
        "  class LSTMTagger(nn.Module):\n",
        "      def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "          super(LSTMTagger, self).__init__()\n",
        "          self.hidden_dim = hidden_dim\n",
        "          self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "          self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "          self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "          self.hidden  = self.init_hidden()\n",
        "          \n",
        "      def init_hidden(self):\n",
        "          return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)), autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
        "          \n",
        "      def forward(self, sentence):\n",
        "          embeds = self.word_embeddings(sentence)\n",
        "          lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1), self.hidden)\n",
        "          tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "          tag_scores = F.log_softmax(tag_space)\n",
        "          return tag_scores\n",
        "      \n",
        "  model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
        "  loss_function = nn.NLLLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "  inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "  tag_scores = model(inputs)\n",
        "  t0 = time.time()\n",
        "  iterations = 300\n",
        "  progress_after = iterations/4\n",
        "\n",
        "  # Training the RNN for 300 iterations\n",
        "  for epoch in range(0,iterations):\n",
        "      for sentence, tags in training_data:\n",
        "          model.zero_grad()\n",
        "          model.hidden = model.init_hidden()\n",
        "          sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "          targets = prepare_sequence(tags, tag_to_ix)\n",
        "          if(progress_after == epoch):\n",
        "              #print(\"Training {}% Completed\".format((progress_after / iterations) * 100))\n",
        "              progress_after = progress_after + (iterations / 4)\n",
        "              \n",
        "          tag_scores = model(sentence_in)\n",
        "          \n",
        "          loss = loss_function(tag_scores, targets)\n",
        "          loss.backward()\n",
        "          optimizer.step() \n",
        "  #print(\"Training Complete!\\nTotal Training time :\", round(time.time()-t0, 2), \"s\\n\")\n",
        "       \n",
        "  inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "  tag_scores = model(inputs)\n",
        "  def generate_questions(model, ref_data_frame):\n",
        "      question = \"\"\n",
        "      sentence = \"\"\n",
        "      index = 0\n",
        "      for index, word_phrase in ref_data_frame.iterrows():\n",
        "          try:\n",
        "              inputs = prepare_sequence([word_phrase[0]], word_to_ix)\n",
        "              tag_scores = model(inputs)\n",
        "          except KeyError:\n",
        "              # ignore\n",
        "              pass\n",
        "\n",
        "          maxVal = max(tag_scores.data.numpy()[0])\n",
        "          index_loc = 0\n",
        "          probability_tag_scores = tag_scores.data.numpy().ravel()\n",
        "\n",
        "          for i in range(0, len(probability_tag_scores)):\n",
        "              if (probability_tag_scores.ravel()[i] == maxVal):\n",
        "                  index_loc = i\n",
        "\n",
        "          for key, value in tag_to_ix.items():\n",
        "              if (value == index_loc):\n",
        "                  sentence = word_phrase[0].split('_')\n",
        "                  if \"NP\" in sentence:\n",
        "                      question = key.replace(\"NP\", sentence[0])\n",
        "                      st.text(question)\n",
        "                  elif \"VP\" in sentence:\n",
        "                      if \"NP\" in key and \"N1\" in key:\n",
        "                          index = [i for i,j in enumerate(subjects.ravel()) if sentence[0] in j]\n",
        "                          ref_subject = subjects.ravel()[index][0].split()\n",
        "                          question = key.replace(\"NP\", ref_subject[0]) \n",
        "                          question = question.replace(\"N1\",ref_subject[-1])\n",
        "                          st.text(question)\n",
        "                      else:\n",
        "                          index = [i for i,j in enumerate(subjects.ravel()) if sentence[0] in j]\n",
        "                          ref_subject = subjects.ravel()[index][0].split()\n",
        "                          question = key.replace(\"NP\", ref_subject[0])\n",
        "                          st.text(question)\n",
        "\n",
        "  generate_questions(model, refferingDataFrame)  \n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting qg.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY-39v_Cze5a"
      },
      "source": [
        "!streamlit run --server.port 80 qg.py&>/dev/null&"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHgqWj9Tzlq6"
      },
      "source": [
        "url = ngrok.connect(port=8501)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuwRXJY3zphh",
        "outputId": "cc764485-5f96-45b2-d2cc-dd2492d70ebf"
      },
      "source": [
        "print(url)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NgrokTunnel: \"http://e0bb97c73fef.ngrok.io\" -> \"http://localhost:80\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3TVbCpcDm1w"
      },
      "source": [
        "ngrok.kill()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygeDqh4p2VL9"
      },
      "source": [
        ""
      ],
      "execution_count": 24,
      "outputs": []
    }
  ]
}